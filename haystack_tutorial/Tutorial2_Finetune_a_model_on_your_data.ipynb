{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJGJ1_1acFZO"
      },
      "source": [
        "# Fine-tuning a Model on Your Own Data\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb)\n",
        "\n",
        "For many use cases it is sufficient to just use one of the existing public models that were trained on SQuAD or other public QA datasets (e.g. Natural Questions).\n",
        "However, if you have domain-specific questions, fine-tuning your model on custom examples will very likely boost your performance.\n",
        "While this varies by domain, we saw that ~ 2000 examples can easily increase performance by +5-20%.\n",
        "\n",
        "This tutorial shows you how to fine-tune a pretrained model on your own dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3G802CUMcFZS"
      },
      "source": [
        "### Prepare environment\n",
        "\n",
        "#### Colab: Enable the GPU runtime\n",
        "Make sure you enable the GPU runtime to experience decent speed in this tutorial.\n",
        "**Runtime -> Change Runtime type -> Hardware accelerator -> GPU**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/colab_gpu_runtime.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLsGtcY4cFZT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# # Make sure you have a GPU running\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNWRnPIocFZU"
      },
      "outputs": [],
      "source": [
        "# # Install the latest release of Haystack in your own environment\n",
        "# #! pip install farm-haystack\n",
        "\n",
        "# # Install the latest main of Haystack\n",
        "# !pip install --upgrade pip\n",
        "# !pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bjJSqo29cFZV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Logging\n",
        "\n",
        "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
        "Example log message:\n",
        "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
        "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DzvIy7RQcFZW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S29kPcmWcFZX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from haystack.nodes import FARMReader\n",
        "from haystack.utils import fetch_archive_from_http"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LgkDlp_cFZY"
      },
      "source": [
        "\n",
        "## Create Training Data\n",
        "\n",
        "There are two ways to generate training data\n",
        "\n",
        "1. **Annotation**: You can use the [annotation tool](https://haystack.deepset.ai/guides/annotation) to label your data, i.e. highlighting answers to your questions in a document. The tool supports structuring your workflow with organizations, projects, and users. The labels can be exported in SQuAD format that is compatible for training with Haystack.\n",
        "\n",
        "![Snapshot of the annotation tool](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/annotation_tool.png)\n",
        "\n",
        "2. **Feedback**: For production systems, you can collect training data from direct user feedback via Haystack's [REST API interface](https://github.com/deepset-ai/haystack#rest-api). This includes a customizable user feedback API for providing feedback on the answer returned by the API. The API provides a feedback export endpoint to obtain the feedback data for fine-tuning your model further.\n",
        "\n",
        "\n",
        "## Fine-tune your model\n",
        "\n",
        "Once you have collected training data, you can fine-tune your base models.\n",
        "We initialize a reader as a base model and fine-tune it on our own custom dataset (should be in SQuAD-like format).\n",
        "We recommend using a base model that was trained on SQuAD or a similar QA dataset before to benefit from Transfer Learning effects.\n",
        "\n",
        "**Recommendation**: Run training on a GPU.\n",
        "If you are using Colab: Enable this in the menu \"Runtime\" > \"Change Runtime type\" > Select \"GPU\" in dropdown.\n",
        "Then change the `use_gpu` arguments below to `True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fauv8QGCcFZa",
        "outputId": "55bd2b6d-f49a-4946-81c9-5a20d3b09aeb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - haystack.modeling.utils -  Using devices: CUDA:0, CUDA:1\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 2\n",
            "INFO - haystack.modeling.utils -  Using devices: CUDA:0, CUDA:1\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 2\n",
            "WARNING - haystack.modeling.infer -  Multiple devices are not supported in Inferencer, using the first device cuda:0.\n",
            "INFO - haystack.modeling.model.language_model -   * LOADING MODEL: 'distilbert-base-uncased-distilled-squad' (DistilBert)\n",
            "INFO - haystack.modeling.model.language_model -  Auto-detected model language: english\n",
            "INFO - haystack.modeling.model.language_model -  Loaded 'distilbert-base-uncased-distilled-squad' (DistilBert model) from model hub.\n",
            "INFO - haystack.modeling.utils -  Using devices: CUDA:0, CUDA:1\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 2\n",
            "WARNING - haystack.modeling.infer -  Multiple devices are not supported in QAInferencer inference, using the first device cuda:0.\n",
            "INFO - haystack.modeling.infer -  Got ya 15 parallel workers to do inference ...\n",
            "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0     0     0     0     0  \n",
            "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\ \n",
            "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   / \\   /'\\   /'\\ \n",
            "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
            "INFO - haystack.modeling.data_handler.data_silo -  \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "INFO - haystack.modeling.data_handler.data_silo -  LOADING TRAIN DATA\n",
            "INFO - haystack.modeling.data_handler.data_silo -  ==================\n",
            "INFO - haystack.modeling.data_handler.data_silo -  Loading train set from: data/squad20/dev-v2.0.json \n",
            "Preprocessing dataset: 100%|██████████| 3/3 [00:06<00:00,  2.06s/ Dicts]\n",
            "INFO - haystack.modeling.data_handler.data_silo -  \n",
            "INFO - haystack.modeling.data_handler.data_silo -  LOADING DEV DATA\n",
            "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
            "INFO - haystack.modeling.data_handler.data_silo -  No dev set is being loaded\n",
            "INFO - haystack.modeling.data_handler.data_silo -  \n",
            "INFO - haystack.modeling.data_handler.data_silo -  LOADING TEST DATA\n",
            "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
            "INFO - haystack.modeling.data_handler.data_silo -  No test set is being loaded\n",
            "INFO - haystack.modeling.data_handler.data_silo -  \n",
            "INFO - haystack.modeling.data_handler.data_silo -  DATASETS SUMMARY\n",
            "INFO - haystack.modeling.data_handler.data_silo -  ================\n",
            "INFO - haystack.modeling.data_handler.data_silo -  Examples in train: 13600\n",
            "INFO - haystack.modeling.data_handler.data_silo -  Examples in dev  : 0\n",
            "INFO - haystack.modeling.data_handler.data_silo -  Examples in test : 0\n",
            "INFO - haystack.modeling.data_handler.data_silo -  Total examples   : 13600\n",
            "INFO - haystack.modeling.data_handler.data_silo -  \n",
            "INFO - haystack.modeling.data_handler.data_silo -  Longest sequence length observed after clipping:     256\n",
            "INFO - haystack.modeling.data_handler.data_silo -  Average sequence length after clipping: 174.0468382352941\n",
            "INFO - haystack.modeling.data_handler.data_silo -  Proportion clipped:      0.12830882352941175\n",
            "INFO - haystack.modeling.model.optimization -  Loading optimizer `AdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "INFO - haystack.modeling.model.optimization -  Multi-GPU Training via DataParallel\n",
            "INFO - haystack.modeling.model.optimization -  Using scheduler 'get_linear_schedule_with_warmup'\n",
            "INFO - haystack.modeling.model.optimization -  Loading schedule `get_linear_schedule_with_warmup`: '{'num_training_steps': 1360, 'num_warmup_steps': 272}'\n",
            "Train epoch 0/0 (Cur. train loss: 0.0000):   0%|          | 0/1360 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\nTypeError: forward() got an unexpected keyword argument 'passage_start_t'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/enjoey/workspace/CSRQA/Tutorial2_Finetune_a_model_on_your_data.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B210.107.197.58/home/enjoey/workspace/CSRQA/Tutorial2_Finetune_a_model_on_your_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/squad20\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B210.107.197.58/home/enjoey/workspace/CSRQA/Tutorial2_Finetune_a_model_on_your_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# data_dir = \"PATH/TO_YOUR/TRAIN_DATA\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B210.107.197.58/home/enjoey/workspace/CSRQA/Tutorial2_Finetune_a_model_on_your_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m reader\u001b[39m.\u001b[39;49mtrain(data_dir\u001b[39m=\u001b[39;49mdata_dir, train_filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdev-v2.0.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_gpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, save_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/haystack/nodes/reader/farm.py:422\u001b[0m, in \u001b[0;36mFARMReader.train\u001b[0;34m(self, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, caching, cache_path, grad_acc_steps, early_stopping)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    348\u001b[0m     data_dir: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m     early_stopping: Optional[EarlyStopping] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    371\u001b[0m ):\n\u001b[1;32m    372\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m    Fine-tune a model on a QA dataset. Options:\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m    - Take a plain language model (e.g. `bert-base-cased`) and train it for QA (e.g. on SQuAD data)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m    :return: None\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_training_procedure(\n\u001b[1;32m    423\u001b[0m         data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m    424\u001b[0m         train_filename\u001b[39m=\u001b[39;49mtrain_filename,\n\u001b[1;32m    425\u001b[0m         dev_filename\u001b[39m=\u001b[39;49mdev_filename,\n\u001b[1;32m    426\u001b[0m         test_filename\u001b[39m=\u001b[39;49mtest_filename,\n\u001b[1;32m    427\u001b[0m         use_gpu\u001b[39m=\u001b[39;49muse_gpu,\n\u001b[1;32m    428\u001b[0m         devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    429\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    430\u001b[0m         n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[1;32m    431\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    432\u001b[0m         max_seq_len\u001b[39m=\u001b[39;49mmax_seq_len,\n\u001b[1;32m    433\u001b[0m         warmup_proportion\u001b[39m=\u001b[39;49mwarmup_proportion,\n\u001b[1;32m    434\u001b[0m         dev_split\u001b[39m=\u001b[39;49mdev_split,\n\u001b[1;32m    435\u001b[0m         evaluate_every\u001b[39m=\u001b[39;49mevaluate_every,\n\u001b[1;32m    436\u001b[0m         save_dir\u001b[39m=\u001b[39;49msave_dir,\n\u001b[1;32m    437\u001b[0m         num_processes\u001b[39m=\u001b[39;49mnum_processes,\n\u001b[1;32m    438\u001b[0m         use_amp\u001b[39m=\u001b[39;49muse_amp,\n\u001b[1;32m    439\u001b[0m         checkpoint_root_dir\u001b[39m=\u001b[39;49mcheckpoint_root_dir,\n\u001b[1;32m    440\u001b[0m         checkpoint_every\u001b[39m=\u001b[39;49mcheckpoint_every,\n\u001b[1;32m    441\u001b[0m         checkpoints_to_keep\u001b[39m=\u001b[39;49mcheckpoints_to_keep,\n\u001b[1;32m    442\u001b[0m         caching\u001b[39m=\u001b[39;49mcaching,\n\u001b[1;32m    443\u001b[0m         cache_path\u001b[39m=\u001b[39;49mcache_path,\n\u001b[1;32m    444\u001b[0m         grad_acc_steps\u001b[39m=\u001b[39;49mgrad_acc_steps,\n\u001b[1;32m    445\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m    446\u001b[0m     )\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/haystack/nodes/reader/farm.py:343\u001b[0m, in \u001b[0;36mFARMReader._training_procedure\u001b[0;34m(self, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, teacher_model, teacher_batch_size, caching, cache_path, distillation_loss_weight, distillation_loss, temperature, tinybert, processor, grad_acc_steps, early_stopping)\u001b[0m\n\u001b[1;32m    324\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer\u001b[39m.\u001b[39mcreate_or_load_checkpoint(\n\u001b[1;32m    325\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    326\u001b[0m         optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    342\u001b[0m \u001b[39m# 5. Let it grow!\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minferencer\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(Path(save_dir))\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/haystack/modeling/training/base.py:212\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Move batch of samples to device\u001b[39;00m\n\u001b[1;32m    211\u001b[0m batch \u001b[39m=\u001b[39m {key: batch[key]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m batch}\n\u001b[0;32m--> 212\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(batch, step)\n\u001b[1;32m    214\u001b[0m \u001b[39m# Perform  evaluation\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_every \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    217\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    218\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    219\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_rank \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    220\u001b[0m ):\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/haystack/modeling/training/base.py:311\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, batch, step)\u001b[0m\n\u001b[1;32m    301\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(\n\u001b[1;32m    302\u001b[0m         query_input_ids\u001b[39m=\u001b[39mbatch[\u001b[39m\"\u001b[39m\u001b[39mquery_input_ids\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    303\u001b[0m         query_segment_ids\u001b[39m=\u001b[39mbatch[\u001b[39m\"\u001b[39m\u001b[39mquery_segment_ids\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m         passage_attention_mask\u001b[39m=\u001b[39mbatch[\u001b[39m\"\u001b[39m\u001b[39mpassage_attention_mask\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m    313\u001b[0m per_sample_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlogits_to_loss(logits\u001b[39m=\u001b[39mlogits, global_step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_propagate(per_sample_loss, step)\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:86\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 86\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     87\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/iknow/anaconda3/envs/cs/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\nTypeError: forward() got an unexpected keyword argument 'passage_start_t'\n"
          ]
        }
      ],
      "source": [
        "reader = FARMReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", use_gpu=True)\n",
        "data_dir = \"data/squad20\"\n",
        "# data_dir = \"PATH/TO_YOUR/TRAIN_DATA\"\n",
        "reader.train(data_dir=data_dir, train_filename=\"dev-v2.0.json\", use_gpu=True, n_epochs=1, save_dir=\"my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bhXFAyncFZb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Saving the model happens automatically at the end of training into the `save_dir` you specified\n",
        "# However, you could also save a reader manually again via:\n",
        "reader.save(directory=\"my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7ZHi_A7cFZb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# If you want to load it at a later point, just do:\n",
        "new_reader = FARMReader(model_name_or_path=\"my_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WIJAZlScFZc"
      },
      "source": [
        "## Distill your model\n",
        "In this case, we have used \"distilbert-base-uncased\" as our base model. This model was trained using a process called distillation. In this process, a bigger model is trained first and is used to train a smaller model which increases its accuracy. This is why \"distilbert-base-uncased\" can achieve quite competitive performance while being very small.\n",
        "\n",
        "Sometimes, however, you can't use an already distilled model and have to distil it yourself. For this case, haystack has implemented [distillation features](https://haystack.deepset.ai/guides/model-distillation).\n",
        "\n",
        "### Augmenting your training data\n",
        "To get the most out of model distillation, we recommend increasing the size of your training data by using data augmentation. You can do this by running the [`augment_squad.py` script](https://github.com/deepset-ai/haystack/blob/main/haystack/utils/augment_squad.py):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYzRPfhDcFZc"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Downloading script\n",
        "subprocess.run([\"wget\",\"https://raw.githubusercontent.com/deepset-ai/haystack/main/haystack/utils/augment_squad.py\"])\n",
        "\n",
        "doc_dir = \"data/tutorial2\"\n",
        "\n",
        "# Downloading smaller glove vector file (only for demonstration purposes)\n",
        "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "fetch_archive_from_http(url=glove_url, output_dir=doc_dir)\n",
        "\n",
        "# Downloading very small dataset to make tutorial faster (please use a bigger dataset for real use cases)\n",
        "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/squad_small.json.zip\"\n",
        "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
        "\n",
        "# Just replace the path with your dataset and adjust the output (also please remove glove path to use bigger glove vector file)\n",
        "subprocess.run([\"python\",\"augment_squad.py\",\n",
        "\"--squad_path\",\"squad_small.json\",\n",
        "\"--output_path\",\"augmented_dataset.json\",\n",
        "\"--multiplication_factor\",\"2\",\n",
        "\"--glove_path\",\"glove.6B.300d.txt\"\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDKKwAnEcFZd"
      },
      "source": [
        "In this case, we use a multiplication factor of 2 to keep this example lightweight. Usually you would use a factor like 20 depending on the size of your training data. Augmenting this small dataset with a multiplication factor of 2, should take about 5 to 10 minutes to run on one V100 GPU.\n",
        "\n",
        "### Running distillation\n",
        "Distillation in haystack is done in two steps: First, you run intermediate layer distillation on the augmented dataset to ensure the two models behave similarly. After that, you run the prediction layer distillation on the non-augmented dataset to optimize the model for your specific task.\n",
        "\n",
        "If you want, you can leave out the intermediate layer distillation step and only run the prediction layer distillation. This way you also do not need to perform data augmentation. However, this will make the model significantly less accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nebEhzjcFZd"
      },
      "outputs": [],
      "source": [
        "# Loading a fine-tuned model as teacher e.g. \"deepset/​bert-​base-​uncased-​squad2\"\n",
        "teacher = FARMReader(model_name_or_path=\"my_model\", use_gpu=True)\n",
        "\n",
        "# You can use any pre-trained language model as teacher that uses the same tokenizer as the teacher model.\n",
        "# The number of the layers in the teacher model also needs to be a multiple of the number of the layers in the student.\n",
        "student = FARMReader(model_name_or_path=\"huawei-noah/TinyBERT_General_6L_768D\", use_gpu=True)\n",
        "\n",
        "student.distil_intermediate_layers_from(teacher, data_dir=\".\", train_filename=\"augmented_dataset.json\", use_gpu=True)\n",
        "student.distil_prediction_layer_from(teacher, data_dir=\"data/squad20\", train_filename=\"dev-v2.0.json\", use_gpu=True)\n",
        "\n",
        "student.save(directory=\"my_distilled_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jO-5KXtMcFZe",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## About us\n",
        "\n",
        "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
        "\n",
        "We bring NLP to the industry via open source!  \n",
        "Our focus: Industry specific language models & large scale QA systems.  \n",
        "  \n",
        "Some of our other work: \n",
        "- [German BERT](https://deepset.ai/german-bert)\n",
        "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
        "- [FARM](https://github.com/deepset-ai/FARM)\n",
        "\n",
        "Get in touch:\n",
        "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Slack](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
        "\n",
        "By the way: [we're hiring!](https://www.deepset.ai/jobs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('cs')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "55c32488f353abc2b13345a23781747ce65d605694f52cfcaabea58a42e43ffa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
